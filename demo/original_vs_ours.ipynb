{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631c3af0",
   "metadata": {},
   "source": [
    "# Original feature vs ours\n",
    "This notebook compares the features from the TSV file, which made with the caffe model, and those from the extractor.   \n",
    "We compare some images in the download directory.\n",
    "The tsv file was made in advance by extracting these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905307a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "from eval_vl_glue import VoltaImageFeature, load_tsv\n",
    "from eval_vl_glue.extractor import BUTDDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cfa473",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    '../download/filled_with_0.png', \n",
    "    '../download/filled_with_255.png',\n",
    "    '../download/000542.jpg', \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc69aa",
   "metadata": {},
   "source": [
    "## Load the TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b465bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_path = '../vl_models/test_obj36.tsv'\n",
    "feature_dict = load_tsv(tsv_path)\n",
    "# Change raw features into inputs for transformers_volta\n",
    "tsv_features = {k: VoltaImageFeature.from_dict(v) for k, v in feature_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74206806",
   "metadata": {},
   "source": [
    "## Configure extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5941af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../download/resnet101_faster_rcnn_final_iter_320000.pt'\n",
    "device = torch.device('cpu')\n",
    "extractor = BUTDDetector()\n",
    "extractor.load_state_dict(torch.load(model_path))\n",
    "extractor = extractor.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75812da9",
   "metadata": {},
   "source": [
    "## Feature matching function\n",
    "- Two tensors have 36 features each (excluding global image feature).     \n",
    "  We match those features trying to minimize the sum of pair distances (cossim, absolute value of differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891294c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to match nearest features in a greedy way\n",
    "def greedy_match(v1, v2, func=None):\n",
    "    func = func or (lambda x, y: (x - y).abs().mean(axis=-1))\n",
    "    ds = func(v1[None], v2[:, None])\n",
    "    n = ds.shape[0]\n",
    "    not_available = ds.max().item() + 100\n",
    "\n",
    "    pairs = []\n",
    "    dists = torch.zeros((n,), dtype=torch.float32)\n",
    "    for _ in range(n):\n",
    "        dists[_] = ds.min().item()\n",
    "        i = ds.argmin().item()\n",
    "        c = int(i % n)\n",
    "        r = int(i // n)\n",
    "        pairs.append((r, c))\n",
    "        ds[r] = not_available\n",
    "        ds[:, c] = not_available\n",
    "    return pairs, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62072fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(v1, v2):\n",
    "    # Element-wise absolute values\n",
    "    pairs, dists = greedy_match(v1, v2, func=lambda x, y: (x - y).abs().mean(axis=-1))\n",
    "    print('absolute mean:', torch.cat([v1, v2], axis=0).abs().mean().item())\n",
    "    print('averaged absolute difference:', dists.mean().item())\n",
    "    print('difference', dists)\n",
    "    \n",
    "    # Vector-wise cosine similarity\n",
    "    pairs, dists = greedy_match(v1, v2, func=lambda x, y: -torch.nn.functional.cosine_similarity(x, y, dim=-1))\n",
    "    dists *= -1\n",
    "    print('averaged cossim:', dists.mean().item())\n",
    "    print('cossim', dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd13784",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24fcbe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taichi/research/eval_vl_glue/venv/lib/python3.9/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "/Users/taichi/research/eval_vl_glue/venv/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../download/filled_with_0.png\n",
      "absolute mean: 0.5030784606933594\n",
      "averaged absolute difference: 0.11184278875589371\n",
      "difference tensor([0.0047, 0.0065, 0.0106, 0.0149, 0.0180, 0.0294, 0.0369, 0.0379, 0.0384,\n",
      "        0.0423, 0.0467, 0.0493, 0.0594, 0.0606, 0.0637, 0.0645, 0.0659, 0.0753,\n",
      "        0.0763, 0.0852, 0.0958, 0.0964, 0.0991, 0.0996, 0.1179, 0.1261, 0.1299,\n",
      "        0.1310, 0.1444, 0.1654, 0.2163, 0.2515, 0.2841, 0.3150, 0.4064, 0.4610])\n",
      "averaged cossim: 0.9530308246612549\n",
      "cossim tensor([1.0000, 0.9999, 0.9998, 0.9996, 0.9996, 0.9986, 0.9982, 0.9979, 0.9972,\n",
      "        0.9967, 0.9963, 0.9956, 0.9953, 0.9953, 0.9928, 0.9927, 0.9922, 0.9916,\n",
      "        0.9909, 0.9888, 0.9872, 0.9825, 0.9807, 0.9801, 0.9783, 0.9766, 0.9729,\n",
      "        0.9721, 0.9537, 0.9377, 0.9047, 0.8851, 0.8584, 0.7946, 0.6990, 0.5267])\n",
      "\n",
      "../download/filled_with_255.png\n",
      "absolute mean: 0.4254292845726013\n",
      "averaged absolute difference: 0.12754279375076294\n",
      "difference tensor([1.9365e-05, 4.1474e-03, 7.3723e-03, 7.9933e-03, 1.0046e-02, 1.4677e-02,\n",
      "        3.2262e-02, 3.4755e-02, 5.3732e-02, 5.4916e-02, 5.5768e-02, 6.1674e-02,\n",
      "        6.2454e-02, 6.8499e-02, 7.5984e-02, 8.1562e-02, 8.2708e-02, 9.2536e-02,\n",
      "        9.4014e-02, 9.4252e-02, 9.5328e-02, 1.1246e-01, 1.2329e-01, 1.2530e-01,\n",
      "        1.2567e-01, 1.4062e-01, 1.4824e-01, 1.9766e-01, 1.9768e-01, 2.2295e-01,\n",
      "        2.2815e-01, 2.4133e-01, 2.5171e-01, 3.8700e-01, 4.6751e-01, 5.3727e-01])\n",
      "averaged cossim: 0.9420244693756104\n",
      "cossim tensor([1.0000, 1.0000, 0.9999, 0.9999, 0.9997, 0.9996, 0.9978, 0.9976, 0.9942,\n",
      "        0.9928, 0.9926, 0.9923, 0.9916, 0.9916, 0.9885, 0.9880, 0.9870, 0.9835,\n",
      "        0.9819, 0.9795, 0.9786, 0.9747, 0.9654, 0.9642, 0.9616, 0.9452, 0.9324,\n",
      "        0.9263, 0.9248, 0.9155, 0.9155, 0.8997, 0.8762, 0.7048, 0.6153, 0.5545])\n",
      "\n",
      "../download/000542.jpg\n",
      "absolute mean: 0.6476185321807861\n",
      "averaged absolute difference: 0.15822933614253998\n",
      "difference tensor([0.0061, 0.0065, 0.0073, 0.0084, 0.0168, 0.0168, 0.0176, 0.0182, 0.0197,\n",
      "        0.0226, 0.0232, 0.0232, 0.0303, 0.0339, 0.0418, 0.0508, 0.0529, 0.0630,\n",
      "        0.0687, 0.0769, 0.1215, 0.1315, 0.1514, 0.1826, 0.1869, 0.2160, 0.2349,\n",
      "        0.2868, 0.3474, 0.3535, 0.3598, 0.4007, 0.4108, 0.4398, 0.5417, 0.7261])\n",
      "averaged cossim: 0.9437382221221924\n",
      "cossim tensor([1.0000, 0.9999, 0.9999, 0.9999, 0.9997, 0.9997, 0.9996, 0.9996, 0.9995,\n",
      "        0.9995, 0.9995, 0.9994, 0.9992, 0.9990, 0.9985, 0.9978, 0.9967, 0.9963,\n",
      "        0.9962, 0.9958, 0.9859, 0.9716, 0.9703, 0.9658, 0.9620, 0.9619, 0.9558,\n",
      "        0.9318, 0.9114, 0.9104, 0.8695, 0.8649, 0.8050, 0.8046, 0.6121, 0.5156])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for image_path in image_paths:\n",
    "    tsv_feature = tsv_features[image_path.split('/')[-1].split('.')[0]]\n",
    "    \n",
    "    # extract features from a image using the extractor\n",
    "    image = PIL.Image.open(image_path)\n",
    "    regions = extractor.detect(image)\n",
    "    ext_feature = VoltaImageFeature.from_regions(regions)\n",
    "    \n",
    "    print(image_path)\n",
    "    # We do not consider global image features.\n",
    "    compare(tsv_feature.features[1:], ext_feature.features[1:])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5c49b",
   "metadata": {},
   "source": [
    "- The averages of element-wise absolute difference ranges 0.1-0.16. These values are approximetly 20% of absolute mean of elements.\n",
    "- The average of vector-wise cosine similarity is approximetly 0.95.\n",
    "- Although two models are similar to some extent, these are not identical. \n",
    "- According to our debug, raw outputs of neural networks, such as rpn_cls_prob_reshape in the forward function, are slightly different and this seems to affect boundary candidates in the remove_small_boxes function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab53f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
