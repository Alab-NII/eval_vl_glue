{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58e52fd",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "This notebook first creates prediction files on the validation set of each task and run.\n",
    "Then it analyze those prediction files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d77a9",
   "metadata": {},
   "source": [
    "## Make prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb181b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate the best models that have the minimum losses for each run\n",
    "# A table best_models.txt will be output in the anaysis directory\n",
    "!python enumerate_best_models.py -r ../../vl_models/finetuned > best_models.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels with those best models\n",
    "# prediction directory will be created in the anaysis directory and \n",
    "# prediction will be stored in the sub-directoryies that corresponds to model directories.\n",
    "# We output strings in dump.log because they are too long to show in a notebook.\n",
    "!./batch_dump.sh best_models.txt > dump.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f205f81",
   "metadata": {},
   "source": [
    "## Calculate the distribution of some metrics\n",
    "We analyze the distributions of sentence lengths and readability here.\n",
    "- Make sentence sets (both solvable, vl-only, bert-only and neither) for each model family from those dump files.\n",
    "- Calculate their statiistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We install packages for this analysis before entering the detail\n",
    "# We use textstat to calculate readability\n",
    "!pip install textstat pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1177b6",
   "metadata": {},
   "source": [
    "### Make sentence sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "# We do not use stsb\n",
    "target_tasks = ['cola', 'sst2', 'mrpc', 'qqp','mnli/mnli_m', 'mnli/mnli_mm', 'qnli',  'rte', 'wnli']\n",
    "# keys for sentences in each task\n",
    "keys_for_tasks = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "    'cc': ('sentence', None)\n",
    "}\n",
    "\n",
    "target_lm = 'bert-base-uncased'\n",
    "target_models = ['ctrl_visual_bert_base',  'ctrl_uniter_base', 'ctrl_vl_bert_base', 'ctrl_lxmert_base_new', 'ctrl_vilbert_base']\n",
    "target_runs = ['1', '2', '3']\n",
    "target_split = 'valid'\n",
    "\n",
    "# path to a directory contains predictions\n",
    "root_path = 'prediction'\n",
    "# path to output the text files of sentence sets\n",
    "output_dir_combined  = 'sentence_sets_combined'\n",
    "# We consider successful if accuracy is higher than this value for each problem.\n",
    "# 0.5 means correct two or three times when the number of runs is three,\n",
    "successful_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa24b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentences from the GLUE tasks\n",
    "datasets = {}\n",
    "for task in target_tasks:\n",
    "    if task.startswith('mnli'):\n",
    "        dataset = load_dataset('glue', 'mnli')\n",
    "        datasets['mnli/mnli_m'] = dataset['validation_matched']\n",
    "        datasets['mnli/mnli_mm'] = dataset['validation_mismatched']\n",
    "    else:\n",
    "        datasets[task] = load_dataset('glue', task)['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5580dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_across_runs(root_path, model, task, runs, split):\n",
    "    \"\"\"This funcition reads the prefiction file of runs and returns their summay.\n",
    "    Returns a list whose items are the summaries of each problem, {_id, sum, total, values}\n",
    "    \"\"\"\n",
    "    # subtask for MNLI's ma and mm\n",
    "    subtask = task\n",
    "    if '/' in task:\n",
    "        task, subtask = task.split('/')\n",
    "    \n",
    "    # is_correct is a dict whose keys are problem_id and \n",
    "    # values are a list of the correctness of runs\n",
    "    is_correct = {}\n",
    "    for run in runs:\n",
    "        file_name = f'{split}_results_{subtask}.txt'\n",
    "        path = os.path.join(root_path, model, task, run, file_name)\n",
    "        with open(path, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            header = next(reader) # ['id', 'predction', 'label']\n",
    "            for row in reader:\n",
    "                is_correct.setdefault(int(row[0]), []).append(row[1]==row[2])\n",
    "    \n",
    "    len_runs = len(runs)\n",
    "    results = []\n",
    "    for key, val in sorted(is_correct.items(), key=lambda kv: kv[0]):\n",
    "        if len(val) != len_runs:\n",
    "            raise Exception(f'total number of runs is not matched for {key}')\n",
    "        results.append({'_id': key, 'sum': sum(val), 'total': len_runs, 'values': val})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_problems(summary_a, summary_b, successful_threshold):\n",
    "    \"\"\"Classify problems based on two summaries.\n",
    "    \"\"\"\n",
    "    both_correct = []\n",
    "    only_a = []\n",
    "    only_b = []\n",
    "    both_wrong = []\n",
    "    for a, b in zip(summary_a, summary_b):\n",
    "        if a['_id'] != b['_id']:\n",
    "            raise Exception('key unmatched')\n",
    "        _id = a['_id']\n",
    "        a_correct = a['sum'] / a['total'] >= successful_threshold\n",
    "        b_correct = b['sum'] / b['total'] >= successful_threshold\n",
    "        if a_correct and b_correct:\n",
    "            both_correct.append(_id)\n",
    "        elif not a_correct and b_correct:\n",
    "            only_b.append(_id)\n",
    "        elif a_correct and not b_correct:\n",
    "            only_a.append(_id)\n",
    "        else:\n",
    "            both_wrong.append(_id)\n",
    "    total = len(both_correct) + len(only_a) + len(only_b) + len(both_wrong)\n",
    "    return {\n",
    "        'total': total,\n",
    "        'both_correct': len(both_correct) / total,\n",
    "        'only_a': len(only_a) / total,\n",
    "        'only_b': len(only_b) / total,\n",
    "        'both_wrong': len(both_wrong) / total,\n",
    "        'ids': (both_correct, only_a, only_b, both_wrong),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conbined_corpus(summary, target_lm, model_name, successful_threshold):\n",
    "    \"\"\"The corpora order is \"both_corpus, only_bert_corpus, only_vl_corpus, neither_corpus\"\n",
    "    \"\"\"\n",
    "    # We output four sets\n",
    "    both_corpus = []\n",
    "    only_bert_corpus = []\n",
    "    only_vl_corpus = []\n",
    "    neither_corpus = []\n",
    "    corpora = [both_corpus, only_bert_corpus, only_vl_corpus, neither_corpus]\n",
    "    \n",
    "    # Combine tasks\n",
    "    for task in summary:\n",
    "        task_name = task.split('/')[0]\n",
    "        ids = classify_problems(summary[task][target_lm], summary[task][model_name], successful_threshold)['ids']\n",
    "        for corpus, problem_ids in zip(corpora, ids):\n",
    "            for problem_id in problem_ids:\n",
    "                for key in keys_for_tasks[task_name]:\n",
    "                    if key is not None:\n",
    "                        corpus.append(datasets[task][problem_id][key])\n",
    "    return corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all summary\n",
    "summary = {}\n",
    "for task in target_tasks:\n",
    "    summary[task] = {}\n",
    "    summary[task][target_lm] = summarize_across_runs(root_path, target_lm, task, target_runs, target_split)\n",
    "    for model_name in target_models:\n",
    "        summary[task][model_name] = summarize_across_runs(root_path, model_name, task, target_runs, target_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sentence sets for all the models.\n",
    "if not os.path.exist(output_dir_combined):\n",
    "    os.mkdir(output_dir_combined)\n",
    "for model_name in target_models:\n",
    "    corpora = make_conbined_corpus(summary, target_lm, model_name, successful_threshold)\n",
    "    filenames = [\n",
    "        f'{model_name}_both.txt', f'{model_name}_bert.txt',\n",
    "        f'{model_name}_vl.txt', f'{model_name}_neither.txt'\n",
    "    ]\n",
    "    for filename, corpus in zip(filenames, corpora):\n",
    "        path = os.path.join(output_dir_combined, filename)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write('\\n'.join(corpus))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5edeb4",
   "metadata": {},
   "source": [
    "### Show statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import pandas\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f15594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data to update font size\n",
    "pandas.DataFrame([0], columns = ['dummy']).hist()\n",
    "matplotlib.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93893f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show_hist(func, prefix, name, ylim=None, xlim=(-5, 20)):\n",
    "    \"\"\" Show a distribution of a metric after applying a function (func) to the sentence in the sets.\n",
    "    We show the distributions of bert-only and vl model in a plot\n",
    "    \"\"\"\n",
    "    plot = None\n",
    "    \n",
    "    for fname, n, color in [\n",
    "        (prefix+'_bert.txt', 'BERT-only', 'r'),\n",
    "        (prefix+'_vl.txt', name, 'b'),\n",
    "    ]:\n",
    "        with open(fname, 'r') as f:\n",
    "            df = pandas.DataFrame((_.strip() for _ in f.readlines()), columns = [n])\n",
    "        x = df[n].apply(func)\n",
    "        print(n, color, len(x), x.mean(), x.std())\n",
    "        plot = x.hist(density=1, range=xlim, bins=40, alpha=0.4, color=color, legend=True)\n",
    "        if ylim is not None:\n",
    "            plot.set_ylim(ylim)\n",
    "    \n",
    "    return plot\n",
    "\n",
    "def show_hist_fkg(prefix, name, ylim=None, xlim=(-5, 20)):\n",
    "    \"\"\"metric is flesch_kincaid_grade\"\"\"\n",
    "    return _show_hist(textstat.flesch_kincaid_grade, prefix, name, ylim, xlim)\n",
    "\n",
    "def show_hist_len(prefix, name, ylim=(0, 0.020), xlim=(0, 500)):\n",
    "    \"\"\"metric is length\"\"\"\n",
    "    return _show_hist(len, prefix, name, ylim, xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca93cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flesch kincaid grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_fkg(f'{output_dir_combined}/ctrl_visual_bert_base', 'VIS-only', ylim=(0, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b66ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_fkg(f'{output_dir_combined}/ctrl_uniter_base', 'UNI-only', ylim=(0, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_fkg(f'{output_dir_combined}/ctrl_vl_bert_base', 'VL-only', ylim=(0, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_fkg(f'{output_dir_combined}/ctrl_lxmert_base_new', 'LX-only',  ylim=(0, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2376bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_fkg(f'{output_dir_combined}/ctrl_vilbert_base', 'VIL-only', ylim=(0, 0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeac154",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_len(f'{output_dir_combined}/ctrl_visual_bert_base', 'VIS-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_len(f'{output_dir_combined}/ctrl_uniter_base', 'UNI-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_len(f'{output_dir_combined}/ctrl_vl_bert_base', 'VL-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_len(f'{output_dir_combined}/ctrl_lxmert_base_new', 'LX-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b050c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist_len(f'{output_dir_combined}/ctrl_vilbert_base', 'VIL-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d29ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
