{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c899ca",
   "metadata": {},
   "source": [
    "# Get GLUE Scores\n",
    "We aggregate evaluation results that exist in each model folder to get the glue score.  \n",
    "The GLUE score for a model is defined the average of task combined scores that are calculated by averaging task metrics.  \n",
    "Our procedure:\n",
    "- Choose the model that has the smallest loss in each run\n",
    "- Calculate the glue combined score for the model\n",
    "- Average the glue combined score of three runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db746eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec7c72",
   "metadata": {},
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c54b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for the GLUE tasks\n",
    "tasks = ['wnli', 'rte', 'mrpc', 'stsb', 'cola', 'sst2', 'qnli', 'qqp', 'mnli']\n",
    "task_metrics = {\n",
    "    'wnli': ['eval_accuracy'],\n",
    "    'rte': ['eval_accuracy'],\n",
    "    'mrpc': ['eval_f1', 'eval_accuracy'],\n",
    "    'stsb': ['eval_pearson', 'eval_spearmanr'],\n",
    "    'cola': ['eval_matthews_correlation'],\n",
    "    'qnli': ['eval_accuracy'],\n",
    "    'sst2': ['eval_accuracy'],\n",
    "    'qqp': ['eval_f1', 'eval_accuracy'],\n",
    "    'mnli': ['eval_accuracy', 'eval_mm_accuracy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(root_dir, prefixes_excluded=[]):\n",
    "    \"\"\"Model directories have their evaluation results in the trainer_state.json file.\n",
    "    This function collects data in trainer_state.json walking sub directories in root_dir.\n",
    "    Returns a dict whose key is a relative path and value is a list of evaluation result on each epoch.\n",
    "    (The timing of evaluation depends on eval_strategy for the run_glue.py)\n",
    "    We assume that a model_path has the format '(prefix)/(vl_model_name)/(task_name)'.\n",
    "    \"\"\"\n",
    "    prefixes_excluded = [os.path.join(root_dir, p) for p in prefixes_excluded]\n",
    "    \n",
    "    data = {}\n",
    "    target_file_name = 'trainer_state.json'\n",
    "    for cur_dir, dirs, files in os.walk(root_dir):\n",
    "        if any(cur_dir.startswith(p) for p in prefixes_excluded):\n",
    "            continue\n",
    "        if target_file_name in files:\n",
    "            results = json.load(open(os.path.join(cur_dir, target_file_name)))\n",
    "            key = cur_dir.replace(root_dir+os.path.sep, '', 1) \n",
    "            data[key] = results.get('log_history', [])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_loss_epoch(data, task_name, skip_columns=['step', 'eval_runtime', 'eval_samples_per_second']):\n",
    "    \"\"\"We assume that a model_path has the format '(prefix)/(vl_model_name)/(task_name)'.\n",
    "    This function filters by task_name and  \n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    rows = []\n",
    "    # Sort to get the same order in terms of model name each time.\n",
    "    for k, v in sorted(data.items(), key=lambda kv: kv[0]):\n",
    "        if task_name and not k.endswith('/'+task_name):\n",
    "            continue\n",
    "        losses = np.asarray([_.get('eval_loss', float('inf')) for _ in v])\n",
    "        max_id = losses.argmin()\n",
    "        best = v[max_id]\n",
    "        if len(columns) == 0:\n",
    "            # In the first loop, we initialize columns (sort for the same order in columns)\n",
    "            columns = [_ for _ in sorted(best.keys()) if _ not in skip_columns]\n",
    "        model_prefix = k.replace('/'+task_name, '')\n",
    "        rows.append([model_prefix]+['%.4f'%best[_] for _ in columns])\n",
    "    columns = ['model'] + columns\n",
    "    return columns, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_model_runs(tasks, task_metrics, table, model_name, only_combined=False, units='raw'):\n",
    "    \"\"\"Calculate the averaged GLUE task and GLUE score of a model specified by model_name.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_statistics_across_runs(array):\n",
    "        # the last axis is for runs\n",
    "        return array.mean(axis=-1), array.std(axis=-1), array.min(axis=-1), array.max(axis=-1)\n",
    "    \n",
    "    scale = 100 if units == '%' else 1\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(('model_name', model_name))\n",
    "    lines.append(('(%s)'%units, 'n', 'avg', 'std', 'min', 'max'))\n",
    "    \n",
    "    # Calculate task scores\n",
    "    glue_score_elements = []\n",
    "    for task_name in tasks:\n",
    "        columns, rows = table[task_name]\n",
    "        model_rows = [row for row in rows if row[0].endswith('/'+model_name)]\n",
    "        num_examples = len(model_rows)\n",
    "        \n",
    "        # Calculate metrics for tasks\n",
    "        metrics_for_a_task = []\n",
    "        for metric in task_metrics[task_name]:\n",
    "            task_col = columns.index(metric)\n",
    "            arr = np.asarray([float(_[task_col])*scale for _ in model_rows])\n",
    "            # The shape of arr = (the number of runs,)\n",
    "            avg, std, _min, _max = get_statistics_across_runs(arr)\n",
    "            # We output the task-specific scores when the only_combined is True.\n",
    "            if not only_combined:\n",
    "                key = task_name + '_' + metric.replace('eval_', '')\n",
    "                lines.append((key, str(num_examples),  '%.4f'%avg, '%.4f'%std, '%.4f'%_min, '%.4f'%_max))\n",
    "            metrics_for_a_task.append(arr)\n",
    "        metrics_for_a_task = np.asarray(metrics_for_a_task)\n",
    "        # The shape of metrics_for_tasks = (the number of metrics, the number of runs)\n",
    "        \n",
    "        # Get combined score for a task by averaging across metrics for the task\n",
    "        combined_score = metrics_for_a_task.mean(axis=0)\n",
    "        avg, std, _min, _max = get_statistics_across_runs(combined_score)\n",
    "        key = task_name + '_' + 'combined'\n",
    "        lines.append((key, str(num_examples), '%.4f'%avg, '%.4f'%std, '%.4f'%_min, '%.4f'%_max))\n",
    "        glue_score_elements.append(arr)\n",
    "    glue_score_elements = np.asarray(glue_score_elements)\n",
    "    # The shape of glue_score_elements = (the number of tasks, the number of runs)\n",
    "    \n",
    "    # Calculate the GLUE score by averaging across tasks\n",
    "    glue_scores = glue_score_elements.mean(axis=0)\n",
    "    avg, std, _min, _max = get_statistics_across_runs(glue_scores)\n",
    "    key = model_name+'_'+'glue'\n",
    "    lines.append((key, str(num_examples), '%.4f'%avg, '%.4f'%std, '%.4f'%_min, '%.4f'%_max))\n",
    "    \n",
    "    return '\\n'.join('\\t'.join(_) for _ in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c026ceb",
   "metadata": {},
   "source": [
    "## Run aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../../../volta_transformers/trained_models'\n",
    "data = collect_data(root_dir)\n",
    "table = {t: get_best_loss_epoch(data, t) for t in tasks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'ctrl_visual_bert_base',\n",
    "    'ctrl_uniter_base',\n",
    "    'ctrl_vl_bert_base',\n",
    "    'ctrl_lxmert_base_new',\n",
    "    'ctrl_vilbert_base',\n",
    "    'ctrl_visual_bert_base_reinit',\n",
    "    'ctrl_uniter_base_reinit',\n",
    "    'ctrl_vl_bert_base_reinit',\n",
    "    'ctrl_lxmert_base_new_reinit',\n",
    "    'ctrl_vilbert_base_reinit',\n",
    "]\n",
    "for model_name in model_names:\n",
    "    results = average_model_runs(\n",
    "        tasks, task_metrics, table, model_name, \n",
    "        only_combined=False, units='%'\n",
    "    )\n",
    "    print(results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d82e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
